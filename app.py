import gradio as gr
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import os
import shutil

model_name = "minoD/JURAN"

# ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="cpu"
)

tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)

# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®æº–å‚™
def generate_prompt(F):
    # input ã‚­ãƒ¼ã®ä»£ã‚ã‚Šã« Q ã¨ F ã‚’ä½¿ç”¨
    result = f"""### æŒ‡ç¤º:ã‚ãªãŸã¯ä¼æ¥­ã®é¢æ¥å®˜ã§ã™ï¼å°±æ´»ç”Ÿã®ã‚¨ãƒ³ãƒˆãƒªãƒ¼ã‚·ãƒ¼ãƒˆã‚’å…ƒã«è³ªå•ã‚’è¡Œã£ã¦ãã ã•ã„ï¼

### è³ªå•:
{F}

### å›ç­”:
""" # å›ç­”ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¿½åŠ 
    # æ”¹è¡Œâ†’<NL>
    result = result.replace('\n', '<NL>')
    return result

# ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆé–¢æ•°ã®å®šç¾©
def generate2(F=None, maxTokens=256):
    # æ¨è«–
    prompt = generate_prompt(F)
    input_ids = tokenizer(prompt,
                          return_tensors="pt",
                          truncation=True,
                          add_special_tokens=False).input_ids
    outputs = model.generate(
        input_ids=input_ids,
        max_new_tokens=maxTokens,
        do_sample=True,
        temperature=0.7,
        top_p=0.75,
        top_k=40,
        no_repeat_ngram_size=2,
    )
    outputs = outputs[0].tolist()
    decoded = tokenizer.decode(outputs)

    # EOSãƒˆãƒ¼ã‚¯ãƒ³ã«ãƒ’ãƒƒãƒˆã—ãŸã‚‰ãƒ‡ã‚³ãƒ¼ãƒ‰å®Œäº†
    if tokenizer.eos_token_id in outputs:
        eos_index = outputs.index(tokenizer.eos_token_id)
        decoded = tokenizer.decode(outputs[:eos_index])

        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹å†…å®¹ã®ã¿æŠ½å‡º
        sentinel = "### å›ç­”:"
        sentinelLoc = decoded.find(sentinel)
        if sentinelLoc >= 0:
            result = decoded[sentinelLoc + len(sentinel):]
            return result.replace("<NL>", "\n")  # <NL>â†’æ”¹è¡Œ
        else:
            return 'Warning: Expected prompt template to be emitted.  Ignoring output.'
    else:
       return 'Warning: no <eos> detected ignoring output'
    
def inference(input_text):
  return generate2(input_text)


iface = gr.Interface(
    fn=inference,
    inputs=gr.Textbox(lines=5, label="å­¦ç”Ÿæ™‚ä»£ã«æ‰“ã¡è¾¼ã‚“ã ã“ã¨ã€ç ”ç©¶ã€ESã‚’å…¥åŠ›", placeholder="åŠå°ä½“ã®ç ”ç©¶ã«æ‰“ã¡è¾¼ã‚“ã "),
    outputs=gr.Textbox(label="æƒ³å®šã•ã‚Œã‚‹è³ªå•"),
    title="JURANğŸŒº",
    description="predict",
)

if __name__ == "__main__":
    iface.launch(share=True)